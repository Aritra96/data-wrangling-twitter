{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Report - Wrangle and Analyze WeRateDogs Data</center>\n",
    "\n",
    "#### <center>Aritra Chattaraj</center>\n",
    "\n",
    "#### <center>April, 2019</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This report is dedicated to documenting all the wrangling efforts that went on into finishing this project. From the kind of skills, patience and perseverance it took to all the times things did work as planned. In essence, this report documents the struggles of data wrangling.\n",
    "\n",
    "I had considered myself pretty proficient with Python until this project came along. It made me realize how little I have (fortunately) had to do when working on kaggle projects or at my workplace where the data is, by and large, pretty neat. This was my first encounter with a truly messy data set and this was both in terms of data quality issues as well as tidyness.\n",
    "\n",
    "All was going well until I faced my first difficulty - reading in the tweets-json.txt file line by line. I tried a number of ways to make this happen but I somehow felt syntactically challenged when most of the my methods failed. Fortunately, I could rely on one of our mentors, Jay T., to help me out of this by giving just the right kind of hint. This made me realize how important a feature the Student Hub is and collaboration in general is wherein you can bounce off your ideas and get help.\n",
    "\n",
    "An interesting part of this project was how data gathering was done differently - first, we had an already .csv file to import, then we had to programmatically download another file and finally, we had to scrape data off of twitter's database. This introduced me to 3 different challenges, all of which were addressed throughout the course of the project.\n",
    "\n",
    "While working with neat and clean rows and columns is a beauty of its own, working with messier rows and columns is no cake-walk. No wonder it is often stated that data clean is over 70% of the job because real data, like the one we got from Twitter, is pretty messy and high in dimension. This quickly got be habituated to programmatically checking, extracting and cleaning the data using Python's functions and methods. Just when you think you're done with cleaning a certain part, you are met with another issue. \n",
    "\n",
    "Finally, I would like to conclude by stating the difficult I faced while merging/joining data frames. While I was able to create a master data frame with two of the files, I could not overcome the challenge of incorporating the third json file into the same .csv file. I tried different methods for merging/joing including concatenation, left joins, outer joins and inner joins but this was definitely something to relook and re-evaluate. \n",
    "\n",
    "Overall, I managed to gather data, assess data, clean data, store data and analyze it (including graphs) to provide insights. I believe this exercise has made me more adept at the aforementioned processes and will go a long way. I learnt a lot of fun insights such as the number of puppers at WeRateDogs or the unique way in which they're rated. I suppose that makes the experience wholesome.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
